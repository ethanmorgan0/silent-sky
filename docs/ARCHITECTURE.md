# Silent Sky Architecture

## Current Architecture (2025)

### Overview

**Silent Sky uses Unity ML-Agents as the authoritative environment**, with Python connecting via ML-Agents Gym interface for training only. This architecture eliminates Python-Unity sync issues and leverages ML-Agents' built-in communication.

### Key Architectural Decision

**Unity is the authoritative environment** - all world model logic (events, sensors, rewards, state) runs in Unity C#. Python focuses solely on RL training via ML-Agents Gym interface.

**Rationale:**
- Unity ML-Agents requires Unity to be the environment
- Single source of truth eliminates sync issues
- ML-Agents handles communication automatically
- Python becomes training-only (Stable-Baselines3 PPO)

### System Components

```
Unity (Authoritative Environment)          Python (Training Only)
├── Environment Logic (C#)                ├── Stable-Baselines3 PPO
│   ├── Event Generation                  │   ├── LSTM Policy
│   ├── Sensor System                     │   └── Training Pipeline
│   ├── Reward Calculation                └── ML-Agents Gym Interface
│   └── State Management                      (connects to Unity)
└── ML-Agents Academy/Agent
    └── Gym Interface (auto-exposed)
```

### Current Implementation Status

**Phase: Pre-ML-Agents (Foundation)**

The world model is currently implemented in Unity as standalone components:

- ✅ **FakeDataGenerator.cs**: Generates events in spherical space
- ✅ **SignalCalculator.cs**: Calculates hexagon signals from events
- ✅ **SpaceEvent.cs**: Event data structure
- ✅ **ViewportProjection.cs**: Sphere-to-viewport projection
- ✅ **ViewportRotationController.cs**: Viewport rotation controls
- ✅ **SphericalCoordinateSystem.cs**: Spherical coordinate utilities
- ✅ **StarfieldBackground.cs**: Procedurally generated starfield
- ✅ **EventVisualizer.cs**: Visualizes events on starfield
- ✅ **SignalVisualizer.cs**: Visualizes signals on hexagons

**Next Phase: ML-Agents Integration**

- ⏳ Install ML-Agents package
- ⏳ Create ObservatoryAcademy (inherits from Academy)
- ⏳ Create ObservatoryAgent (inherits from Agent)
- ⏳ Port environment logic to ML-Agents structure
- ⏳ Connect Python training via ML-Agents Gym interface

### Coordinate System

**Spherical Coordinates:**
- Theta (θ): Azimuth angle [0, 2π] (longitude)
- Phi (φ): Polar angle [0, π] (latitude)
- All events and positions use (theta, phi) coordinates

**Two-Step Mapping:**
1. **Sphere → Viewport**: Equirectangular projection (180° × 120° FOV)
2. **Viewport → Hexagons**: Point-in-hexagon test for 19 hexagons

**Hexagon Layout:**
- 19 hexagons in JWST-style honeycomb pattern
- 1 center + 6 in ring 1 + 12 in ring 2
- Hexagons represent fixed regions of the sphere

### Viewport System

**Viewport Rotation:**
- Viewport can rotate around sphere (theta_offset, phi_offset)
- Hexagons stay fixed on sphere, viewport rotates
- Events rotate with viewport (they're on the sphere)
- Rotation controlled by ViewportRotationController

**FOV:**
- 180° horizontal × 120° vertical (human-like FOV)
- Configurable via ViewportProjection constants

### Event System

**Current Implementation:**
- Events generated by FakeDataGenerator
- Events have: type, value, theta, phi, timestamp, duration
- Events are active during [timestamp, timestamp + duration]
- Signals calculated by summing event values per hexagon

**Future (ML-Agents):**
- Event generation will move to ML-Agents environment
- Events will be part of ground truth state
- Agent will only see noisy sensor readings (POMDP)

### Sensor System

**Current:**
- Signals are sum of event values (ground truth)
- No noise yet (pre-ML-Agents)

**Future (ML-Agents):**
- Sensor system will add noise to ground truth
- Agent observations will be noisy (POMDP enforcement)
- Sensor confidence will be sensor metadata only (not belief state)

### POMDP Design

**Critical Rules:**
- Agent never receives ground truth
- Only noisy sensor readings in observations
- Sensor confidence is sensor metadata (not belief state)
- No event history in observations (LSTM learns this)
- No sector activity estimates (agent infers)
- No prediction hints (upgrade outputs only, not observations)

**Observation Space (Future):**
- `sensor_readings`: Box(0.0, 1.0, (19,)) - noisy sensor data per hexagon
- `sensor_confidence`: Box(0.0, 1.0, (19,)) - sensor noise metadata
- `time_remaining`: Box(0.0, 1.0, (1,)) - normalized time
- `budget_remaining`: Box(0.0, 1.0, (1,)) - normalized budget

**Action Space (Future):**
- Discrete: Sector selection (0-18)
- Future: Add exposure mode (SHORT/MEDIUM/LONG)

### Reward System

**Future (ML-Agents):**
- `discovery_value`: Based on event type and observation quality
- `operational_cost`: Based on actions taken
- `efficiency_bonus`: Small bonus for optimal resource usage (optional)
- `information_gain`: Small bonus for reducing uncertainty (optional)

**Critical Rule:**
- RL reward ≠ money directly
- Agent never sees money in observations
- Money is for player display only

### Migration Path

**Current State → ML-Agents:**

1. ✅ World model foundation in Unity (FakeDataGenerator, SignalCalculator)
2. ⏳ Install ML-Agents package
3. ⏳ Create Academy/Agent structure
4. ⏳ Port environment logic to ML-Agents
5. ⏳ Implement POMDP observations
6. ⏳ Connect Python training
7. ⏳ Remove/update ZMQBridge (replaced by ML-Agents)

### File Structure

```
unity/Silent Sky/Assets/Scripts/
├── Environment/
│   ├── FakeDataGenerator.cs          ✅ Current (pre-ML-Agents)
│   ├── SignalCalculator.cs            ✅ Current (pre-ML-Agents)
│   ├── SpaceEvent.cs                  ✅ Current
│   ├── ViewportProjection.cs          ✅ Current
│   ├── ViewportRotationController.cs   ✅ Current
│   ├── SphericalCoordinateSystem.cs   ✅ Current
│   ├── StarfieldBackground.cs         ✅ Current
│   ├── EventVisualizer.cs              ✅ Current
│   ├── SignalVisualizer.cs             ✅ Current
│   ├── ObservatoryAcademy.cs           ⏳ Future (ML-Agents)
│   ├── ObservatoryAgent.cs             ⏳ Future (ML-Agents)
│   ├── EnvironmentState.cs             ⏳ Future (ML-Agents)
│   ├── EventSystem.cs                  ⏳ Future (ML-Agents)
│   ├── SensorSystem.cs                 ⏳ Future (ML-Agents)
│   └── RewardSystem.cs                 ⏳ Future (ML-Agents)
└── Visualization/
    ├── SectorMap.cs                    ✅ Current
    ├── SphereMinimap.cs                ✅ Current
    └── ...
```

### Communication

**Current:**
- ZMQBridge exists but uses mock data
- No real Python-Unity connection yet

**Future (ML-Agents):**
- ML-Agents handles communication automatically
- Python connects via `gym.make()` with ML-Agents environment
- No ZMQ needed (ML-Agents replaces it)

### Determinism

**Critical Requirement:**
- All random number generation must be seedable
- Same seed = identical episodes
- Use Unity's `Random.State` for deterministic RNG
- Essential for reproducibility and replay

### Notes

- The current implementation is a **foundation** for ML-Agents
- World model logic exists but isn't yet in ML-Agents structure
- Next step: Integrate existing components into ML-Agents Academy/Agent
- Python environment code (observatory_env.py) exists but is deprecated
- Future: Python will only contain training code, not environment logic

